<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2017%2F10%2F11%2Fmysql%20question%2F</url>
    <content type="text"><![CDATA[数据库问题集锦1. mysql赋予用户权限 （1）问题：SQLException: access denied for @’localhost’ (using password: no) 答案： grant all privileges on . to ‘root’@’%’ identified by ‘123456’; grant 权限1，权限2 on 数据库名.表名 to 用户名@用户地址 identified by ‘连接口令’;flush privileges;exit 注：权限1,权限2,…权限n代表select,insert,update,delete,create,drop,index,alter,grant,references,reload,shutdown,process,file等14个权限。 2. 用户密码修改 （1）使用mysqladmin命令–适用于记得root旧密码，修改root密码 语法：mysqladmin -u用户名 -p旧密码 password 新密码 例如：mysqladmin -u root -proot password mysql]]></content>
  </entry>
  <entry>
    <title><![CDATA[磁盘文件系统的选择]]></title>
    <url>%2F2017%2F09%2F25%2FDisk%20format%20choise%2F</url>
    <content type="text"><![CDATA[磁盘文件系统的选择引言我们在Windows系统里格式化磁盘的时候，文件系统的选项里可以看到有“FAT”、“FAT32”、“NTFS”、“REFS”等选项，在对U盘或其他移动存储设备 格式化的时候还会出现“exFAT”选项，那么这四种磁盘格式是什么意思，有哪些优缺点呢？我们应该选择那个呢？ 介绍FAT（FAT16）文件系统（1）它采用16位的文件分配表，能支持最大为2GB的硬盘分区。 （2）FAT文件系统优点：FAT（FAT16）是目前应用最为广泛和获得操作系统最多的一种磁盘分区格式，几乎所有的操作系统都支持这一种格式，从DOS WINDOWS95/98/NT/ME/2000/XP，甚至LINUX都支持这种分区格式。 （3）FAT文件系统缺点：最大只支持2GB的分区，而且每个分区最多只能有65525个簇。因此磁盘利用效率非常 低。因为在DOS和WINDOWS系统中，磁盘文件的分配是以簇为单位的，一个簇只分配给一个文件使用，不管这个文件占用整个簇容量的多少。这样，即使一 个文件很小的话，它也要占用一个簇，剩余的空间便全部闲置在那里，形成了磁盘空间的浪费。由于分区表容量的限制，FAT的分区越大，磁盘上每个簇的容量也 越大，造成的浪费也越大。FAT文件系统，目前除了一些特殊应用之外，基本上已经不再使用了。试想下，目前主流的硬盘容量已经达到1TB（1TB=1024GB）了，假如使用FAT格式的话，每个分区最大只能2GB，那么电脑里面会有多少个盘符呀！ FAT32文件系统（1）FAT32格式采用32位的文件分配表，使其对磁盘 的能力大大增强，突破了FAT16对每一个分区的容量只有2GB的限制。Win95以上的操作系统都支持FAT32格式。 （2）FAT32文件系统优点：突破了FAT对每一个分区的容量只有2GB的限制，可以支持大到2TB（2048G）的分区。在不超过8GB的分区容量下，每个簇的容量都固定为4KB，与FAT16相比，可以大大减少磁盘的浪费，提高磁盘利用率。 （3）FAT32文件系统缺点：用 FAT32格式分区的磁盘，由于文件分配表的扩大，运行速度比采用FAT16格式分区的磁盘要慢，且DOS系统和某些早期的应用软件不支持这种分区格式。 另外还有一个致命的缺点让FAT32逐渐被淘汰，这就是FAT32的单个文件最大只能支持4GB。现在已经进入高清时代，720P和1080P高清视频文件都很容易超过4GB，因此FAT32已经走向没落。 NTFS文件系统（1）NTFS是从Windows XP系统开始逐渐成为主流的磁盘格式，是微软Windows NT内核的系列操作系统支持的、一个特别为网络和磁盘配额、文件加密等管理安全特性设计的磁盘格式。支持NTFS磁盘格式的操作系统有：WINDOWS NT、WINDOWS2000、WINDOWS2003、WINDOWS XP、WINDOWS vista、WINDOWS7等。因此NTFS目前仍是主流的磁盘格式，有大量用户在使用。 （2）NTFS文件系统优点：NTFS分区具有极高的安全性和稳定性，在使用中不易产生文件碎片。它能对用户的操作 进行记录，通过对用户权限进行非常严格的限制，使每个用户只能按照系统赋予的权限进行操作，充分保护了系统与数据的安全。另外对大部分用户而言，NTFS 最直观的优点是，单个文件的大小突破了FAT32的4GB的限制。 （3）NTFS文件系统缺点：NTFS虽然有诸多优点，但这些都是针对传统机械硬盘而设计的，对于新兴的Flash 闪存材料不一定适用。NTFS分区是采用“日志式”的文件系统，因为要记录磁盘的详细读写操作，对U盘这种闪存储介质会造成较大的负担，比如同样存取一个 文件或目录，在NTFS系统上的读写次数就会比FAT32来得多，理论上NTFS格式的U盘比较容易损坏，而且400MB以下的分区也比FAT16更浪费 空间。 （4）用途：NTFS是为机械硬盘准备的格式，OS X虽然不能写但是可以读。NTFS是日志式，即使硬盘有坏道也能及时抢救内部数据。如有需要可以在Mac上装Paragon NTFS for Mac OS X。 REFS文件系统（1）ReFS最初是由Windows 8和Server 2012引进的，ReFS是下一代的Windows文件系统，据消息在下一个Win10重大版本，即Win10秋季创意者更新中将默认使用ReFS。 NTFS已经是目前较为成熟的文件系统，ReFS的优势在于能够提供更高的稳定性，可以自动验证数据是否损坏，并尽力恢复数据。但目前尚不支持引导系统，也不能用于移动存储设备。 exFAT文件系统（1）exFAT是近年才出现的格式，主要针对移动存储设备，什么闪存、U盘等。因为FAT32格式单个文件不能超过4G，使用NTFS格式又容易损坏闪存芯片，所以才开发EXFAT格式来解决这些问题。 （2）exFAT文件系统优点：分区大小和单文件大小最大可达16EB（16×1024×1024TB）；簇大小非 常灵活，最小0.5KB，最高达32MB；采用了剩余空间分配表，空间利用率更高；同一目录下最大文件数可达65536个；支持访问控制；支持 TFAT(WINCE早期文件系统)。可以看出，ExFAT就是闪存专用的文件系统，只有U盘和存储卡才能格式化成exFAT，传统硬盘是无法格式化成 exFAT格式的，因为exFAT的特性其实并不比NTFS强，但却比NTFS及FAT32更适合闪存使用。 （3）exFAT文件系统缺点：exFAT作为一种全新的文件系统，在电脑上的兼容性却不太好，曾经的XP和Vista默认都不支持ExFAT，XP需升级至SP3补丁、Vista需升级至SP1补丁才能支持它。目前的Win7、10默认支持。 （4）用途：ExFat为闪存而准备的硬盘格式，SSD或者U盘用还行，机械硬盘用就不行了，可能会掉数据。]]></content>
      <categories>
        <category>磁盘文件系统</category>
      </categories>
      <tags>
        <tag>硬件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署Ceph及与OpenStack结合]]></title>
    <url>%2F2017%2F09%2F12%2F%E9%83%A8%E7%BD%B2Ceph%E5%8F%8A%E4%B8%8EOpenStack%E7%BB%93%E5%90%88%2F</url>
    <content type="text"><![CDATA[声明：本安装文档以CentOS 7+ 或 Ubuntu 14+为基础 本文转载自王宇霄博客，感谢霄哥指导前期准备在安装ceph之前推荐把所有的ceph节点设置成无需密码ssh互访，配置hosts支持主机名互访，同步好时间，并关闭iptables和selinux。deploy节点是配置ceph，并不安装ceph实验环境： 1controller deploy &amp; OSD 2compute Monitor &amp; OSD 3compute Monitor &amp; OSD 4compute Monitor &amp; OSD xvda 系统盘 xvdb、xvdc、xvde sata盘 xvdf ssd (ceph-deploy命令就是在deploy节点操作) 调整文件连接数的限制(all)CentOS系统 执行修改 12345echo '* soft nofile 65535' &gt;&gt; /etc/security/limits.confecho '* hard nofile 65535' &gt;&gt; /etc/security/limits.confecho 'ulimit -SHn 65535' &gt;&gt; /etc/profileulimit -SHn 65535sed -i -e 's/4096/unlimited/g' /etc/security/limits.d/20-nproc.conf 检查配置 1ulimit -n Ubuntu系统 执行修改 1234echo '* soft nofile 65535' &gt;&gt; /etc/security/limits.confecho '* hard nofile 65535' &gt;&gt; /etc/security/limits.confecho 'ulimit -SHn 65535' &gt;&gt; /etc/profileulimit -SHn 65535 检查配置 1ulimit -n 安装ssh server(all)—-看情况定CentOS系统1sudo yum install openssh-server Ubuntu系统1sudo apt install openssh-server 更新防火墙(all)CentOS系统 关闭防火墙(方式一) 12systemctl stop firewalld.servicesystemctl disable firewalld.service 在测试过程中，关闭防火墙是一个不错的建议，但在实际生产环境中，这是极不安全的，推荐启用防火墙开放端口(方式二)$ systemctl start firewalld.service$ systemctl enable firewalld.service 1234# Ceph's Monitor节点默认使用端口: 6789firewall-cmd --zone=public --add-service=ceph-mon --permanent# Ceph's OSD及MDS节点默认使用端口范围: [6800, 7300]firewall-cmd --zone=public --add-service=ceph --permanent 针对 12345678910# 使用网络接口名称代替&lt;IFACE&gt;# 使用集群的网络号代替&lt;IP&gt;# 使用子网掩码数代替&lt;NETMASK&gt;iptables -A INPUT -i IFACE -p tcp -s IP/NETMASK --dport 6789 -j ACCEPTiptables -A INPUT -i IFACE -p tcp -s IP/NETMASK --dport 6800:7300 -j ACCEPT# 持久化规则mkdir -p /etc/iptables/iptables-save &gt; /etc/iptables/rules.v4echo 'iptables-restore -c /etc/iptables/rules.v4' &gt;&gt; /etc/profile Ubuntu系统（Ubuntu默认安装是没有开启任何防火墙的） 开放端口(方式一) 1234567apt install -y firewalld# Ceph's Monitor节点默认使用端口: 6789firewall-cmd --zone=public --add-port=6789/tcp --permanent# Ceph's OSD节点默认使用端口范围: [6800, 7300]firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent# 使配置生效firewall-cmd --reload 针对（方式二） 12345678910# 使用网络接口名称代替&lt;IFACE&gt;# 使用集群的网络号代替&lt;IP&gt;# 使用子网掩码数代替&lt;NETMASK&gt;iptables -A INPUT -i IFACE -p tcp -s IP/NETMASK --dport 6789 -j ACCEPTiptables -A INPUT -i IFACE -p tcp -s IP/NETMASK --dport 6800:7300 -j ACCEPT# 持久化规则mkdir -p /etc/iptables/iptables-save &gt; /etc/iptables/rules.v4echo 'iptables-restore -c /etc/iptables/rules.v4' &gt;&gt; /etc/profile 关闭SELinux(CentOS系统)CentOS系统12setenforce 0sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config 配置主机名及解析（注意不能使用ceph命令及ceph_deploy这种带下划线的）1vim /etc/hostname 1vim /etc/hosts 主机间免密通信（deploy节点）生成密钥1ssh-keygen -t rsa -P "" 拷贝公钥到其他节点1234ssh-copy-id root@&#123;HostName&#125;ssh-copy-id root@172.18.50.91ssh-copy-id root@172.18.50.92ssh-copy-id root@172.18.50.93 安装NTP服务（all）CentOS系统1yum install -y chrony 1vim /etc/chrony.conf 1server cn.pool.ntp.org iburst 12\cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtimesystemctl restart chronyd.service Ubuntu系统1apt install -y chrony 1vim /etc/chrony/chrony.conf 1server cn.pool.ntp.org iburst 12\cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeservice chrony restart 安装部署导入密钥(All Node)CentOS系统12rpm --import 'http://mirrors.ustc.edu.cn/ceph/keys/release.asc'rpm --import 'http://handge.cn:10080/ceph/keys/release.asc' Ubuntu系统1wget -q -O- 'http://mirrors.ustc.edu.cn/ceph/keys/release.asc' | sudo apt-key add - 添加软件源(All Node)CentOS系统1yum install -y epel-release 12vim /etc/yum.repos.d/ceph.repowget http://handge.cn:10080/repos.d/CentOS-Ceph.repo -O /etc/yum.repos.d/CentOS-Ceph.repo 12345678[ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.ustc.edu.cn/ceph/rpm-jewel/el7/noarchenabled=1priority=1gpgcheck=1type=rpm-mdgpgkey=http://mirrors.ustc.edu.cn/ceph/keys/release.asc 1yum makecache &amp;&amp; yum update -y Ubuntu系统1echo deb http://mirrors.ustc.edu.cn/ceph/debian-jewel/ $(lsb_release -sc) main | tee /etc/apt/sources.list.d/ceph.list 1apt update &amp;&amp; apt dist-upgrade -y 安装Deploy工具(Deploy Node)CentOS系统12yum install -y ceph-deploypip install ceph-deploy Ubuntu系统1apt install -y ceph-deploy 替换Deploy的软件源(Deploy Node)CentOS系统123456echo 'export CEPH_DEPLOY_REPO_URL=http://mirrors.ustc.edu.cn/ceph/rpm-jewel/el7' &gt;&gt; /etc/profileecho 'export CEPH_DEPLOY_GPG_URL=http://mirrors.ustc.edu.cn/ceph/keys/release.asc' &gt;&gt; /etc/profileecho 'export CEPH_DEPLOY_REPO_URL=http://handge.cn:10080/ceph/rpm-jewel/el7' &gt;&gt; /etc/profileecho 'export CEPH_DEPLOY_GPG_URL=http://handge.cn:10080/ceph/keys/release.asc' &gt;&gt; /etc/profilesource /etc/profile Ubuntu系统123456echo 'export CEPH_DEPLOY_REPO_URL=http://mirrors.ustc.edu.cn/ceph/debian-jewel' &gt;&gt; /etc/profileecho 'export CEPH_DEPLOY_GPG_URL=http://mirrors.ustc.edu.cn/ceph/keys/release.asc' &gt;&gt; /etc/profileecho 'export CEPH_DEPLOY_REPO_URL=http://172.18.20.100/ceph/debian-jewel' &gt;&gt; /etc/profileecho 'export CEPH_DEPLOY_GPG_URL=http://172.18.20.100/ceph/keys/release.asc' &gt;&gt; /etc/profilesource /etc/profile 准备工作(controller)创建目录12mkdir -p /openstack/ceph-clustercd /openstack/ceph-cluster 开始部署(deploy节点)新建Monitor节点并生成配置文件及密钥(个数为 2n + 1 )12ceph-deploy new &#123;NodeName&#125; //奇数个Monitor节点就写几个NodeNameceph-deploy new compute&#123;1,2,3&#125; 1检查当前目录是否增加ceph.conf、ceph.mon.keyring等 编辑配置文件1vim /openstack/ceph-cluster/ceph.conf 参考配置文件 安装Ceph12ceph-deploy install &#123;NodeName&#125; //几个Ceph节点就写几个NodeNameceph-deploy install controller compute&#123;1,2,3&#125; 初始化Monitor节点并收集密钥12ceph-deploy --overwrite-conf mon create-initialps:注意 如果此过程失败，并显示类似于“无法找到/etc/ceph.client.admin.keyring”的消息，请确保ceph.conf中监视节点列出的IP是公用IP，而不是私有IP 部署OSD列出指定节点的磁盘信息12345ceph-deploy disk list &#123;NodeName&#125;ceph-deploy disk list compute&#123;1,2,3&#125;orssh root@&#123;NodeName&#125; lsblk 格式化指定节点的磁盘12345ceph-deploy disk zap &#123;NodeName&#125;:&#123;DiskName&#125;如：ceph-deploy disk zap controller:/dev/xvdb controller:/dev/xvdc controller:/dev/xvde controller:/dev/xvdfceph-deploy disk zap compute1:/dev/xvdb compute1:/dev/xvdc compute1:/dev/xvde compute1:/dev/xvdf 准备OSD磁盘(Sata磁盘)123456# JournalDisk为日志存储的位置, 一般使用SSD磁盘ceph-deploy osd prepare &#123;NodeName&#125;:&#123;Sata_Disk_Name&#125;[:&#123;Journal_Disk&#125;]ceph-deploy osd prepare &#123;NodeName&#125;:&#123;Sata_Disk_Name&#125;[:&#123;SSD_Disk_Name&#125;]ceph-deploy osd prepare controller:/dev/xvdb:/dev/xvdf controller:/dev/xvdc:/dev/xvdf controller:/dev/xvde:/dev/xvdfceph-deploy osd prepare compute1:/dev/xvdb:/dev/xvdf compute1:/dev/xvdc:/dev/xvdf compute1:/dev/xvde:/dev/xvdf 划分Ceph缓存区(SSD磁盘)（all ceph节点）将SSD磁盘剩余的磁盘作为缓存OSD，其中{END}为上一个分区的结束标志123456lsblkparted /dev/&#123;SSD_Disk_Name&#125;(parted) print(parted) mkpart cache xfs &#123;END&#125; -1 1为结尾，-1为开头(parted) print(parted) quit 准备OSD磁盘(SSD磁盘)12345ceph-deploy osd prepare &#123;NodeName&#125;:&#123;SSD_Disk_Name&#125;&#123;Index&#125;# 示例ceph-deploy osd prepare controller:/dev/xvdf4ceph-deploy osd prepare compute1:/dev/xvdf4 激活OSD磁盘(Sata磁盘)123456ceph-deploy osd activate &#123;NodeName&#125;:&#123;Sata_Disk_Name&#125;&#123;Index&#125;:&#123;SSD_Disk_Name&#125;&#123;Index&#125;# 示例四个osd节点依次激活ceph-deploy osd activate controller:/dev/xvdb1:/dev/xvdf1 controller:/dev/xvdc1:/dev/xvdf2 controller:/dev/xvde1:/dev/xvdf3ceph-deploy osd activate compute1:/dev/xvdb1:/dev/xvdf1 compute1:/dev/xvdc1:/dev/xvdf2 compute1:/dev/xvde1:/dev/xvdf3 激活OSD磁盘(SSD磁盘)123ceph-deploy osd activate &#123;NodeName&#125;:&#123;SSD_Disk_Name&#125;&#123;Index&#125;# 示例ceph-deploy osd activate controller:/dev/xvdf4 compute1:/dev/xvdf4 compute2:/dev/xvdf4 compute3:/dev/xvdf4 为磁盘添加标签(All Node)列出Ceph的所有磁盘1ceph-disk list 设置SSD-OSD的标签为Ceph-Data（4个ceph节点分别执行）$lsblk1234# 格式（typecode根据磁盘排序，xvda为0，xvdb为1）sgdisk --typecode=&lt;Index&gt;:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- &#123;SSD_Disk_Name&#125;# 示例sgdisk --typecode=4:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/xvdf 设置SSD-OSD的标签为Ceph-Journal(可选操作–在kolla时可选)1234# 格式（typecode根据磁盘排序，xvda为0，xvdb为1）sgdisk --typecode=&lt;Index&gt;:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- &#123;SSD_Disk_Name&#125;# 示例sgdisk --typecode=1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 -- /dev/xvdf 通过命令$lsblk查看osd挂载情况 手动编辑设置映射(在安装monitor的节点操作)12345678910111213141516171819# 导出密文ceph osd getcrushmap -o crushmap.dump# 转换明文crushtool -d crushmap.dump -o crushmap.txt#在各cepf节点通过命令lsblk查看各个osd挂载情况# 编辑&lt;crushmap.txt&gt;, 请参阅外部文件（里面的id不能重复,host为安装ceph的节点）vim crushmap.txt在一个monitor节点修改后，会在导入之后自动分发各节点# 生成密文crushtool -c crushmap.txt -o crushmap.done# 导入密文ceph osd setcrushmap -i crushmap.done# 删除文件rm -f crushmap.* 查看crushmap效果图 完成安装分发配置及密钥(deploy节点，进入指定的/openstack/ceph-cluster/)123ceph-deploy --overwrite-conf admin &#123;NodeName&#125;(NodeName是安装ceph节点)如：ceph-deploy --overwrite-conf admin controller compute1 compute2 compute3 获取集群的健康度123ceph -sceph -wceph health 删除默认的Pool, 版本不再自动创建123cepf osd lspools 查看poolsceph osd pool ls detailceph osd pool delete rbd rbd --yes-i-really-really-mean-it 配置Ceph为OpenStack的后端存储ps:ceph缓存分层技术：通过用作现有较大池的缓存的快速存储设备来提高IO性能。它使用快速/昂贵的存储设备（主要是现在的SSD）创建一个池。这被称为缓存层。现有的后备池可以是擦除编码的池或由较慢/更便宜的存储设备组成的复制池。这被称为存储层或基础层。缓存层保存基础层中数据的子集。 创建’OSD-Pool’（一个ceph节点）123456789101112131415161718192021222324252627282930313233343536373839# 若少于5个OSD, 设置pg_num为128;# 5~10个OSD，设置pg_num为512;# 10~50个OSD，设置pg_num为4096;# 超过50个OSD, 根据(PG数 = OSD数*100/副本数/POOL数)来计算;# 4096为pg_numceph osd pool create images 4096 ceph osd pool create images.cache 4096ceph osd pool create volumes 4096ceph osd pool create volumes.cache 4096ceph osd pool create vms 4096ceph osd pool create vms.cache 4096ceph osd pool create backups 4096ceph osd pool create backups.cache 4096#查看结果ceph osd lspools### 更新相应Pool的属性值(备用)```bash$ ceph osd pool set images pg_num 512$ ceph osd pool set volumes pg_num 512$ ceph osd pool set vms pg_num 512$ ceph osd pool set backups pg_num 512$ ceph osd pool set images pgp_num 512$ ceph osd pool set volumes pgp_num 512$ ceph osd pool set vms pgp_num 512$ ceph osd pool set backups pgp_num 512```bash### 删除&lt;OSD-Pool&gt;将OpenStack中已存在的VM、Image与Vloume清理掉（备用）```bashceph osd pool delete images images --yes-i-really-really-mean-itceph osd pool delete volumes volumes --yes-i-really-really-mean-itceph osd pool delete vms vms --yes-i-really-really-mean-itceph osd pool delete backups backups --yes-i-really-really-mean-itceph osd pool delete images.cache images.cache --yes-i-really-really-mean-itceph osd pool delete volumes.cache volumes.cache --yes-i-really-really-mean-itceph osd pool delete vms.cache vms.cache --yes-i-really-really-mean-itceph osd pool delete backups.cache backups.cache --yes-i-really-really-mean-it 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 将&lt;Pool&gt;加入不同的&lt;CRUSH Map&gt;规则集中(crush_ruleset为crushmap中的参数)# 后备存储-SATA盘ceph osd pool set images crush_ruleset 0ceph osd pool set volumes crush_ruleset 0ceph osd pool set vms crush_ruleset 0ceph osd pool set backups crush_ruleset 0# 缓存存储-SSD盘ceph osd pool set images.cache crush_ruleset 1ceph osd pool set volumes.cache crush_ruleset 1ceph osd pool set vms.cache crush_ruleset 1ceph osd pool set backups.cache crush_ruleset 1# 将缓存层与后备存储池相关联ceph osd tier add images images.cacheceph osd tier add volumes volumes.cacheceph osd tier add vms vms.cacheceph osd tier add backups backups.cache# 设置缓存模式(热存储回写)ceph osd tier cache-mode images.cache writebackceph osd tier cache-mode volumes.cache writebackceph osd tier cache-mode vms.cache writebackceph osd tier cache-mode backups.cache writeback# 高速缓存层覆盖后备存储池, 直接将客户端流量引导到缓存池ceph osd tier set-overlay images images.cacheceph osd tier set-overlay volumes volumes.cacheceph osd tier set-overlay vms vms.cacheceph osd tier set-overlay backups backups.cache# 启用缓存存储池的命中集跟踪ceph osd pool set images.cache hit_set_type bloomceph osd pool set volumes.cache hit_set_type bloomceph osd pool set vms.cache hit_set_type bloomceph osd pool set backups.cache hit_set_type bloom# 为缓存存储池保留的命中集数量（hit_set_count定义了每一个命中设置的时间应该覆盖多少秒，而hit_set_period定义了有多少这样的攻击集被持久化。默认值为4个命中集，每一个命中集合包含1200秒。）ceph osd pool set images.cache hit_set_count 12ceph osd pool set volumes.cache hit_set_count 12ceph osd pool set vms.cache hit_set_count 12ceph osd pool set backups.cache hit_set_count 12# 为缓存存储池保留的命中集有效期ceph osd pool set images.cache hit_set_period 14400ceph osd pool set volumes.cache hit_set_period 14400ceph osd pool set vms.cache hit_set_period 14400ceph osd pool set backups.cache hit_set_period 14400# 达到多少字节时回写数据ceph osd pool set images.cache target_max_bytes 53687091200ceph osd pool set volumes.cache target_max_bytes 53687091200ceph osd pool set vms.cache target_max_bytes 53687091200ceph osd pool set backups.cache target_max_bytes 53687091200ceph osd pool set images.cache target_max_objects 1000000ceph osd pool set volumes.cache target_max_objects 1000000ceph osd pool set vms.cache target_max_objects 1000000ceph osd pool set backups.cache target_max_objects 1000000# 保留访问记录, 取值越高, 消耗的内存就越多（ps: min_read_recency_for_promote / min_write_recency_for_promote越高，ceph -osd守护进程消耗的RAM越多）ceph osd pool set images.cache min_read_recency_for_promote 2ceph osd pool set images.cache min_write_recency_for_promote 2ceph osd pool set volumes.cache min_read_recency_for_promote 2ceph osd pool set volumes.cache min_write_recency_for_promote 2ceph osd pool set vms.cache min_read_recency_for_promote 2ceph osd pool set vms.cache min_write_recency_for_promote 2ceph osd pool set backups.cache min_read_recency_for_promote 2ceph osd pool set backups.cache min_write_recency_for_promote 2 (controller节点)12345678# Nova用户, 允许使用&lt;volumes&gt;、&lt;vms&gt;、&lt;images&gt;池ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images ,allow rwx pool=volumes.cache, allow rwx pool=vms.cache, allow rwx pool=images.cache'# Cinder用户, 允许使用&lt;volumes&gt;、&lt;backups&gt;池ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=volumes.cache ,allow rwx pool=backups ,allow rwx pool=backups.cache'# Glance用户, 允许使用&lt;images&gt;池ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images, allow rwx pool=images.cache' 获取认证密钥controller节点12345678ceph auth get-or-create client.nova | ssh root@controller tee /etc/ceph/ceph.client.nova.keyringssh root@controller chown nova:nova /etc/ceph/ceph.client.nova.keyringceph auth get-or-create client.cinder | ssh root@controller tee /etc/ceph/ceph.client.cinder.keyringssh root@controller chown cinder:cinder /etc/ceph/ceph.client.cinder.keyringceph auth get-or-create client.glance | ssh root@controller tee /etc/ceph/ceph.client.glance.keyringssh root@controller chown glance:glance /etc/ceph/ceph.client.glance.keyring Compute节点12345ceph auth get-or-create client.nova | ssh root@&#123;ComputeNode&#125; tee /etc/ceph/ceph.client.nova.keyringssh root@&#123;ComputeNode&#125; chown nova:nova /etc/ceph/ceph.client.nova.keyringceph auth get-or-create client.nova | ssh root@compute1 tee /etc/ceph/ceph.client.nova.keyringssh root@compute1 chown nova:nova /etc/ceph/ceph.client.nova.keyring 配置Nova(All Compute节点)配置nova的libvirt认证获取密钥1ceph auth get-key client.nova | tee ~/nova.key 生成UUID, 用于认证12345UUID=$(uuidgen)orUUID=$(cat /etc/ceph/ceph.conf | grep "fsid" | awk -F " = " '&#123;print $2&#125;') 生成文件12345678cat &gt; ~/secret.xml &lt;&lt;EOF&lt;secret ephemeral='no' private='no'&gt; &lt;uuid&gt;$&#123;UUID&#125;&lt;/uuid&gt; &lt;usage type='ceph'&gt; &lt;name&gt;client.nova secret&lt;/name&gt; &lt;/usage&gt;&lt;/secret&gt;EOF 定义文件1virsh secret-define --file ~/secret.xml 列出已定义的项目1virsh secret-list 取消定义(备用)1virsh secret-undefine $&#123;UUID&#125; 关联密钥与UUID1virsh secret-set-value --secret $&#123;UUID&#125; --base64 $(cat ~/nova.key) &amp;&amp; rm -f ~/&#123;nova.key,secret.xml&#125; 配置nova配置文件1vim /etc/nova/nova.conf 1234567891011[libvirt]images_type = rbdimages_rbd_pool = vmsimages_rbd_ceph_conf = /etc/ceph/ceph.confrbd_user = novarbd_secret_uuid = UUIDdisk_cachemodes=&quot;network=writeback&quot;inject_password = trueinject_key = trueinject_partition = -2live_migration_flag=&quot;VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED&quot; 1sed -i "s#UUID#$&#123;UUID&#125;#g" /etc/nova/nova.conf 重启服务1234ubuntu系统：service nova-compute restartcentos系统：systemctl restart libvirtd.service openstack-nova-compute.service 配置Glance(Controller节点)编辑配置文件1vim /etc/glance/glance-api.conf 1234567891011121314151617181920[DEFAULT]show_image_direct_url = True[glance_store]stores = rbddefault_store =rbdrbd_store_pool = imagesrbd_store_user = glancerbd_store_ceph_conf = /etc/ceph/ceph.confrbd_store_chunk_size = 8[task] (找到后直接添加)task_executor = taskflowwork_dir=/tmp[taskflow_executor] (找到后直接添加)engine_mode = serialmax_workers = 10conversion_format=raw 重启服务1234service glance-api restartservice glance-registry restartsystemctl restart openstack-glance-api.service openstack-glance-registry.service 配置Cinder(Controller节点)编辑配置文件1vim /etc/cinder/cinder.conf 12345678910111213141516171819202122[DEFAULT] （直接添加)enabled_backends = cephbackup_driver = cinder.backup.drivers.cephbackup_ceph_conf = /etc/ceph/ceph.confbackup_ceph_user = cinder-backupbackup_ceph_chunk_size = 134217728backup_ceph_pool = backupsbackup_ceph_stripe_unit = 0backup_ceph_stripe_count = 0restore_discard_excess_bytes = true[ceph]volume_driver = cinder.volume.drivers.rbd.RBDDriverrbd_pool = volumesrbd_ceph_conf = /etc/ceph/ceph.confrbd_flatten_volume_from_snapshot = falserbd_max_clone_depth = 5rbd_store_chunk_size = 4rados_connect_timeout = -1glance_api_version = 2rbd_user = cinderrbd_secret_uuid = UUID 1sed -i "s#UUID#$&#123;UUID&#125;#g" /etc/cinder/cinder.conf 重启服务12345678service cinder-api restartservice cinder-scheduler restartservice cinder-volume restartservice cinder-backup restartservice tgt restartcentos系统：systemctl restart openstack-cinder-volume.service target.service 结果查看 测试操作获取Ceph的存储使用率1ceph df 下载镜像源1wget http://172.18.20.100/img/raw/ubuntu/ubuntu-16.04-server.raw 上传镜像1openstack image create "ubuntu-16" --file ubuntu-16.04-server.raw --disk-format raw --container-format bare --public 获取镜像1openstack image list 获取Ceph的存储使用率1ceph df]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[采用virt-install方式制作OpenStack镜像]]></title>
    <url>%2F2017%2F08%2F28%2F%E5%88%B6%E4%BD%9COpenStack%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[声明：本文档以Ubuntu14.04为基础ps:如果您不希望使用virt-manager（例如，您不想在服务器上安装依赖项，则您没有在本地运行X服务器，X11通过SSH转发不起作用），您可以使用virt-install工具通过libvirt启动虚拟机，并从安装在本地计算机上的VNC客户端连接到图形控制台。 制作Ubuntu镜像ps:制作centos则采用qemu方式速度太慢，故制作ubuntu镜像参考 检测系统是否支持KVM（数字为0表示不支持，反之亦然）1$ egrep -c '(vmx|svm)' /proc/cpuinfo 更新软件包索引与更新软件1$ apt update &amp;&amp; apt dist-upgrade -y]]></content>
      <categories>
        <category>OpenStack</category>
      </categories>
      <tags>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Docker]]></title>
    <url>%2F2017%2F08%2F28%2F%E5%AE%89%E8%A3%85Docker%2F</url>
    <content type="text"><![CDATA[声明：本安装文档以Ubuntu14.04为基础 前期准备1.更新系统1$ apt-get update 2.安装CA证书1$ apt-get install apt-transport-https ca-certificates 3.添加一个新的GPG键（GPG是加密和数字签名的免费工具，大多用于加密信息的传递）1$ apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D 4.添加源12vim /etc/apt/sources.list.d/docker.listdeb https://apt.dockerproject.org/repo ubuntu-trusty main 5.更新系统1$ apt-get update 6.获取可用的Docker版本1$ apt-cache policy docker-engine 7.安装推荐的包1$ apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual 8.安装额外的包1$ apt-get install linux-image-generic-lts-trusty 9.对包进行更新1$ apt-get update 安装docker1.对包进行更新1$ apt-get update 2.安装引1$ apt-get install docker-engine 配置DaoCloud的Docker加速器(国内registry-mirror)1.DaoCloud的Docker Hub Mirror服务可代替Docker官网的Docker Hub登录DaoCloud官网,注册用户并登录 2.获取添加加速器命令登录以后进入管理界面点击“加速器”标签，会弹出页面配置加速器中产生的命令，然后将此命令复制到linux的命令行中执行 3.重启Docker服务1$ service docker restart 验证操作1.查看docker版本1$ docker -v 2.启动docker守护进程1$ service docker start 3.验证docker是否正确安装1$ docker run hello-world ps:此命令下载的是一个测试镜像，它打印“Hello from Docker!”消息结束后退出。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub+Hexo的配置和使用]]></title>
    <url>%2F2017%2F07%2F05%2FGitHubPages%26HexoGuide%2F</url>
    <content type="text"><![CDATA[安装前提安装 Hexo 相当简单。然而在安装前，您必须检查电脑中是否已安装下列应用程序： Node.js Git 如果您的电脑中已安装 Node.js , Git 的话，那么接下来就要用npm来安装Hexo了 GitHub Pages 仓库在自己的GitHub账号下创建一个新的仓库，命名为username.github.io（username是你的账号名)。 在这里，要知道，GitHub Pages有两种类型：User/Organization Pages 和 Project Pages，而我所使用的是User Pages。 简单来说，User Pages 与 Project Pages的区别是： User Pages 是用来展示用户的，而 Project Pages 是用来展示项目的。 用于存放 User Pages 的仓库必须使用username.github.io的命名规则，而 Project Pages 则没有特殊的要求。 User Pages 将使用仓库的 master 分支，而 Project Pages 将使用 gh-pages 分支。 User Pages 通过 http(s)://username.github.io 进行访问，而 Projects Pages通过 http(s)://username.github.io/projectname 进行访问。 Git配置name和email当安装完Git应该做的第一件事情就是设置用户名称和邮件地址。这样做很重要，因为每一个Git的提交都会使用这些信息，并且它会写入你的每一次提交中，不可更改：12$ git config --global user.name &quot;username&quot;$ git config --global user.email &quot;username@example.com&quot; 查看git配置可以使用 -l 参数(l 就是 list 的首字母,L的小写):1$ git config -l 在某个项目根路径下面可以设置单独的Email与姓名.12$ git config user.name &quot;tiemaocsdn&quot;$ git config user.email &quot;tiemaocsdn@qq.com&quot; ssh配置查看是否已经有了ssh密钥：cd ~/.ssh 如果没有密钥则不会有此文件夹，有则备份删除 生存密钥：1$ ssh-keygen -t rsa -C &quot;username@example.com&quot; 按3个回车，密码为空。 最后得到了两个文件：id_rsa和id_rsa.pub 添加密钥到ssh：ssh-add id_rsa 需要之前输入密码。 在github上添加ssh密钥，这要添加的是“id_rsa.pub”里面的公钥 git上进行测试1$ ssh git@github.com 返回结果123PTY allocation request failed on channel 0 Hi username! You&apos;ve successfully authenticated, but GitHub does not provide shell access. Connection to github.com closed. github ssh配置完毕 Hexo安装如果上述必备程序已完成，那么接下来即可使用npm完成Hexo的安装1$ npm install -g hexo-cli Hexo建站安装Hexo完成后，Hexo需要在指定的文件夹init初始化，在你喜欢的文件夹内（例如D：\Hexo），点击鼠标右键选择Git bash，输入以下指令：1$ hexo init 接下来是安装依赖包，该命令会在当前文件夹内建立网站所需要的所有文件：1$ npm install 新建完成后，当前文件夹的目录如下：123456.├── _config.yml 网站的 配置 信息，您可以在此配置大部分的参数├── package.json 应用程序的信息├── scaffolds 模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件├── source 资源文件夹是存放用户资源的地方└── themes 主题文件夹。Hexo 会根据主题来生成静态页面。 现在可以通过下面的命令，搭建到本地预览一下12$ hexo generate$ hexo server 然后在浏览器输入localhost:4000查看 这个博客只是本地的，别人是浏览不了的，之后需要部署到GitHub上。 Hexo配置您可以在 _config.yml 中修改大部份的配置 找到这一个部分12deploy: type: 然后在github上仓库的ssh地址复制过来，修改后1234deploy: type: git repo: 对应仓库的SSH地址（可以在GitHub对应的仓库中复制） branch: 分支（User Pages为master，Project Pages为gh-pages） 为了能够使Hexo部署到GitHub上，需要安装一个插件：1$ npm install hexo-deployer-git --save 然后通过下面的命令发布到github的仓库上完成部署：12$ hexo generate$ hexo deploy theme配置如果想要使用其他主题，可以使用git clone将别人的主题拷贝到站点目录的\themes下，然后将_config.yml中的theme: landscape改为对应的主题名字 个人推荐一个不错的主题：Next 详细步骤可以参考Next官网。 命令常用命令1234567$ npm install -g hexo-cli$ hexo init$ npm install$ hexo generate$ hexo server$ hexo clean$ hexo deploy 其他命令12345$ hexo new [layout] &lt;title&gt;$ hexo publish [layout] &lt;filename&gt;$ hexo render &lt;file1&gt; [file2] ...$ hexo list &lt;type&gt;$ hexo version]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>GitHub</tag>
        <tag>配置</tag>
      </tags>
  </entry>
</search>
